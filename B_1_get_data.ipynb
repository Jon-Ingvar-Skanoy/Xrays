{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = max(1, os.cpu_count() // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine: DESKTOP-UHDJ875, User: jonal, OS: Linux\n"
     ]
    }
   ],
   "source": [
    "machine_name = platform.node()  \n",
    "user = os.getenv(\"USER\") or os.getenv(\"USERNAME\") \n",
    "os_name = platform.system()  # Get os\n",
    "print(f\"Machine: {machine_name}, User: {user}, OS: {os_name}\")\n",
    "\n",
    "if machine_name == \"Corsair\" and os_name == \"Linux\" and user == \"jon\":\n",
    "    windows_drive = Path(\"/mnt/b/Xray\")\n",
    "    paths = {\n",
    "        \"dataset\": windows_drive / \"dataset\",\n",
    "        \"tar_images\": windows_drive / \"dataset/images\",\n",
    "        \"images\": windows_drive / \"dataset/images/images\",\n",
    "        \"checkpoints\": windows_drive / \"checkpoints\",\n",
    "        \"papers\": windows_drive / \"papers\",\n",
    "        \"models\": windows_drive / \"models\",\n",
    "    }\n",
    "\n",
    "    batch_size = 64\n",
    "\n",
    "else:\n",
    "    dataset_dir = Path(\"dataset\")\n",
    "    paths = {\n",
    "        \"dataset\": dataset_dir,\n",
    "        \"tar_images\": dataset_dir / \"images\",\n",
    "        \"images\": dataset_dir / \"images/images\",\n",
    "        \"checkpoints\": dataset_dir / \"checkpoints\",\n",
    "        \"papers\": dataset_dir / \"papers\",\n",
    "        \"models\": dataset_dir / \"models\",\n",
    "    }\n",
    "\n",
    "for key, path in paths.items():\n",
    "    path.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dir_tar_images = paths['tar_images']\n",
    "\n",
    "path_dataset = paths['dataset']\n",
    "path_images = paths['images']\n",
    "path_csv_list = paths['dataset'] / \"Data_Entry_2017_v2020.csv\"\n",
    "path_train_val_list = paths['dataset'] / \"train_val_list.txt\"\n",
    "path_test_list = paths['dataset'] / \"test_list.txt\"\n",
    "path_models = paths['models']\n",
    "checkpoint_dir = paths['checkpoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def download_file(link, folder, idx):\n",
    "    \"\"\"\n",
    "    Downloads a file from a link to the specified folder.\n",
    "    \"\"\"\n",
    "    file_name = f'images_{idx+1:03d}.tar.gz'\n",
    "    file_path = os.path.join(folder, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        #print(f\"{file_name} already exists, skipping download.\")\n",
    "        return file_path\n",
    "    try:\n",
    "        print(f\"Downloading {file_name}...\")\n",
    "        urllib.request.urlretrieve(link, file_path)\n",
    "        print(f\"{file_name} downloaded successfully.\")\n",
    "        return file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {file_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_file(file_path, folder):\n",
    "    \"\"\"\n",
    "    Extracts a .tar.gz file to the specified folder.\n",
    "    \"\"\"\n",
    "    extracted_flag = file_path.replace('.tar.gz', '_extracted.flag')\n",
    "    if os.path.exists(extracted_flag):\n",
    "        #print(f\"{os.path.basename(file_path)} already extracted, skipping.\")\n",
    "        return\n",
    "    try:\n",
    "        print(f\"Extracting {os.path.basename(file_path)}...\")\n",
    "        with tarfile.open(file_path, 'r:gz') as tar:\n",
    "            tar.extractall(path=folder)\n",
    "        with open(extracted_flag, 'w') as f:\n",
    "            f.write('extracted')\n",
    "        print(f\"{os.path.basename(file_path)} extracted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract {os.path.basename(file_path)}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def process_link(idx, link):\n",
    "    \"\"\"\n",
    "    Handles downloading and extracting a single link.\n",
    "    \"\"\"\n",
    "    file_path = download_file(link, dir_tar_images, idx)\n",
    "    if file_path:\n",
    "        extract_file(file_path, dir_tar_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images_001.tar.gz...\n",
      "Downloading images_002.tar.gz...\n",
      "Downloading images_003.tar.gz...\n",
      "Downloading images_004.tar.gz...\n",
      "Downloading images_005.tar.gz...\n",
      "Downloading images_006.tar.gz...\n",
      "Downloading images_007.tar.gz...\n",
      "Downloading images_008.tar.gz...\n",
      "images_001.tar.gz downloaded successfully.\n",
      "Extracting images_001.tar.gz...\n",
      "images_004.tar.gz downloaded successfully.\n",
      "Extracting images_004.tar.gz...\n",
      "images_008.tar.gz downloaded successfully.\n",
      "Extracting images_008.tar.gz...\n",
      "images_007.tar.gz downloaded successfully.\n",
      "Extracting images_007.tar.gz...\n",
      "images_002.tar.gz downloaded successfully.\n",
      "Extracting images_002.tar.gz...\n",
      "images_006.tar.gz downloaded successfully.\n",
      "Extracting images_006.tar.gz...\n",
      "images_005.tar.gz downloaded successfully.\n",
      "Extracting images_005.tar.gz...\n",
      "images_003.tar.gz downloaded successfully.\n",
      "Extracting images_003.tar.gz...\n",
      "images_001.tar.gz extracted successfully.\n",
      "Downloading images_009.tar.gz...\n",
      "images_009.tar.gz downloaded successfully.\n",
      "Extracting images_009.tar.gz...\n",
      "images_004.tar.gz extracted successfully.\n",
      "Downloading images_010.tar.gz...\n",
      "images_002.tar.gz extracted successfully.\n",
      "Downloading images_011.tar.gz...\n",
      "images_008.tar.gz extracted successfully.\n",
      "Downloading images_012.tar.gz...\n",
      "images_007.tar.gz extracted successfully.\n",
      "images_005.tar.gz extracted successfully.\n",
      "images_006.tar.gz extracted successfully.\n",
      "images_003.tar.gz extracted successfully.\n",
      "images_012.tar.gz downloaded successfully.\n",
      "Extracting images_012.tar.gz...\n",
      "images_010.tar.gz downloaded successfully.\n",
      "Extracting images_010.tar.gz...\n",
      "images_011.tar.gz downloaded successfully.\n",
      "Extracting images_011.tar.gz...\n",
      "images_009.tar.gz extracted successfully.\n",
      "images_012.tar.gz extracted successfully.\n",
      "images_010.tar.gz extracted successfully.\n",
      "images_011.tar.gz extracted successfully.\n",
      "Download and extraction complete. Please check the extracted files.\n"
     ]
    }
   ],
   "source": [
    "links = [\n",
    "    'https://nihcc.box.com/shared/static/vfk49d74nhbxq3nqjg0900w5nvkorp5c.gz',\n",
    "    'https://nihcc.box.com/shared/static/i28rlmbvmfjbl8p2n3ril0pptcmcu9d1.gz',\n",
    "    'https://nihcc.box.com/shared/static/f1t00wrtdk94satdfb9olcolqx20z2jp.gz',\n",
    "\t'https://nihcc.box.com/shared/static/0aowwzs5lhjrceb3qp67ahp0rd1l1etg.gz',\n",
    "    'https://nihcc.box.com/shared/static/v5e3goj22zr6h8tzualxfsqlqaygfbsn.gz',\n",
    "\t'https://nihcc.box.com/shared/static/asi7ikud9jwnkrnkj99jnpfkjdes7l6l.gz',\n",
    "\t'https://nihcc.box.com/shared/static/jn1b4mw4n6lnh74ovmcjb8y48h8xj07n.gz',\n",
    "    'https://nihcc.box.com/shared/static/tvpxmn7qyrgl0w8wfh9kqfjskv6nmm1j.gz',\n",
    "\t'https://nihcc.box.com/shared/static/upyy3ml7qdumlgk2rfcvlb9k6gvqq2pj.gz',\n",
    "\t'https://nihcc.box.com/shared/static/l6nilvfa9cg3s28tqv1qc1olm3gnz54p.gz',\n",
    "\t'https://nihcc.box.com/shared/static/hhq8fkdgvcari67vfhs7ppg2w6ni4jze.gz',\n",
    "\t'https://nihcc.box.com/shared/static/ioqwiy20ihqwyr8pf4c24eazhh281pbu.gz'\n",
    "]\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    executor.map(lambda args: process_link(*args), enumerate(links))\n",
    "\n",
    "print(\"Download and extraction complete. Please check the extracted files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_xrays",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
