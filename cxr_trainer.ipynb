{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from filelock import FileLock\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, roc_auc_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models import list_models, get_model, get_model_weights\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from torch.amp import GradScaler, autocast\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "from datetime import datetime\n",
    "import time\n",
    "import hashlib\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "use_amp = True\n",
    "scaler = torch.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "script_dir = os.getcwd()\n",
    "user = os.getenv(\"USER\") \n",
    "if user == \"jon\":\n",
    "    script_dir = \"/mnt/b/Xray\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = f\"{script_dir}/dataset/data/train\"\n",
    "test_dir = f\"{script_dir}/dataset/data/test\"\n",
    "labels_file = f\"{script_dir}/dataset/Data_Entry_2017_v2020.csv\"\n",
    "cache_dir = \"cache\"\n",
    "models_dir = f\"{script_dir}/models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "results_dir = f\"{script_dir}/results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "detailed_results_path = f\"{results_dir}/detailed_model_results_{timestamp}.csv\"\n",
    "summary_results_path = f\"{results_dir}/summary_model_results_{timestamp}.csv\"\n",
    "locks_dir = \"locks\"  # Directory for lock files\n",
    "os.makedirs(locks_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = max(1, os.cpu_count() // 2)\n",
    "prefetch_factor = 3\n",
    "enable_cache = True\n",
    "rebuild_cache = False\n",
    "num_train_images = None#1000 #None\n",
    "num_test_images = None#200 #None\n",
    "checkpoint_interval = 10\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['densenet121', 'densenet161', 'densenet169', 'densenet201', \n",
    "               'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7',\n",
    "               'resnet101', 'resnet152', 'resnext101_32x8d',\n",
    "               'convnext_base', 'convnext_large', \n",
    "               'vit_b_16', 'swin_b', 'resnet50', 'mobilenet_v3_large', 'googlenet', 'inception_v3',\n",
    "               'vgg16', 'vgg19', 'alexnet']\n",
    "\n",
    "runs_per_model = 5\n",
    "lock_timeout = 86400 # 24 hours 604800 # 1 week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ChestXray14CachedDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for loading cached tensors and multi-label vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, label_mapping, pathologies):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset (list): List of (patient_id, cached_tensor_path) pairs.\n",
    "            label_mapping (dict): Dictionary mapping image paths to label vectors.\n",
    "            pathologies (list): List of pathologies for model alignment.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.label_mapping = label_mapping\n",
    "        self.pathologies = pathologies\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_id, tensor_path = self.dataset[idx]\n",
    "        image = torch.load(tensor_path, weights_only=True)  # Set weights_only=True for security\n",
    "        image_name = os.path.basename(tensor_path).replace(\".pt\", \"\")\n",
    "        labels = self.label_mapping[image_name]\n",
    "        label_vector = torch.tensor([labels[self.pathologies.index(p)] for p in self.pathologies], dtype=torch.float)\n",
    "        return {\"img\": image, \"lab\": label_vector}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterable(iterable, batch_size):\n",
    "    \"\"\"Yield successive batches from an iterable.\"\"\"\n",
    "    iterator = iter(iterable)\n",
    "    while True:\n",
    "        batch = list(islice(iterator, batch_size))\n",
    "        if not batch:\n",
    "            break\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save(dataset, transform, cache_dir=\"cache\", num_workers=4, batch_size=32, enable_cache=True, rebuild_cache=False):\n",
    "    \"\"\"\n",
    "    Preprocess and save dataset images in batches, with optional caching and multiprocessing.\n",
    "\n",
    "    Args:\n",
    "        dataset (list): List of (patient_id, image_path) pairs.\n",
    "        transform (callable): Transformations to apply to the images.\n",
    "        cache_dir (str): Directory to store cached preprocessed images.\n",
    "        num_workers (int): Number of parallel workers for preprocessing.\n",
    "        batch_size (int): Number of items to process in each batch.\n",
    "        enable_cache (bool): If True, use caching; otherwise, process all files without caching.\n",
    "        rebuild_cache (bool): If True, overwrite existing cache files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (patient_id, cached_image_path or transformed_image) pairs.\n",
    "    \"\"\"\n",
    "    if not enable_cache:\n",
    "        print(\"Caching is disabled. Processing images in memory.\")\n",
    "    \n",
    "    print(\"\\nBuilding cache...\")\n",
    "    if enable_cache:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "        # Clear cache directory if rebuilding\n",
    "        if rebuild_cache:\n",
    "            print(f\"Rebuilding cache. Clearing directory: {cache_dir}\")\n",
    "            for file in os.listdir(cache_dir):\n",
    "                file_path = os.path.join(cache_dir, file)\n",
    "                os.remove(file_path)\n",
    "\n",
    "    def process_batch(batch):\n",
    "        results = []\n",
    "        for patient_id, image_path in batch:\n",
    "            cache_path = os.path.join(cache_dir, f\"{os.path.basename(image_path)}.pt\") if enable_cache else None\n",
    "            if not enable_cache or rebuild_cache or (enable_cache and not os.path.exists(cache_path)):\n",
    "                try:\n",
    "                    image = Image.open(image_path).convert(\"RGB\")\n",
    "                    image = transform(image)\n",
    "                    if enable_cache:\n",
    "                        torch.save(image, cache_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "            results.append((patient_id, cache_path if enable_cache else image))\n",
    "        return results\n",
    "\n",
    "    def worker(input_queue, output_queue):\n",
    "        while True:\n",
    "            batch = input_queue.get()\n",
    "            if batch is None:  # End of queue signal\n",
    "                break\n",
    "            output_queue.put(process_batch(batch))\n",
    "\n",
    "    # Create queues\n",
    "    input_queue = mp.Queue()\n",
    "    output_queue = mp.Queue()\n",
    "\n",
    "    # Start worker processes\n",
    "    workers = []\n",
    "    for i in range(num_workers):\n",
    "        print(f\"Starting worker process {i+1}/{num_workers}\", end=\"\\r\")\n",
    "        process = mp.Process(target=worker, args=(input_queue, output_queue))\n",
    "        process.start()\n",
    "        workers.append(process)\n",
    "    print()\n",
    "\n",
    "    # Divide dataset into batches and add to queue\n",
    "    total_batches = (len(dataset) + batch_size - 1) // batch_size\n",
    "    for i, batch in enumerate(batch_iterable(dataset, batch_size)):\n",
    "        print(f\"Adding batches to queue: {i+1}/{total_batches}\", end=\"\\r\")\n",
    "        input_queue.put(batch)\n",
    "    print()\n",
    "\n",
    "    # Signal workers to terminate\n",
    "    for i in range(num_workers):\n",
    "        input_queue.put(None)\n",
    "\n",
    "    # Collect results\n",
    "    preprocessed_dataset = []\n",
    "    start_time = time.time()\n",
    "    for i in range(total_batches):\n",
    "        batch_start = time.time()\n",
    "        preprocessed_dataset.extend(output_queue.get())\n",
    "        batch_end = time.time()\n",
    "        \n",
    "        # Calculate elapsed time and remaining time\n",
    "        elapsed_time = batch_end - start_time\n",
    "        batches_processed = i + 1\n",
    "        avg_batch_time = elapsed_time / batches_processed\n",
    "        remaining_time = avg_batch_time * (total_batches - batches_processed)\n",
    "        eta = time.strftime('%H:%M:%S', time.gmtime(remaining_time))\n",
    "        \n",
    "        print(f\"Collecting results: {batches_processed}/{total_batches}, ETA: {eta}\", end=\"\\r\")\n",
    "    print()\n",
    "\n",
    "    # Wait for workers to finish\n",
    "    for process in workers:\n",
    "        process.join()\n",
    "\n",
    "    print(f\"Preprocessing complete. Total processed items: {len(preprocessed_dataset)}\")\n",
    "    return preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(directory, random_selection=False, seed=None, max_total_images=None):\n",
    "    if random_selection and seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    patient_images = defaultdict(list)\n",
    "\n",
    "    for filename in sorted(os.listdir(directory)):\n",
    "        if filename.endswith(\".png\"):\n",
    "            patient_id = filename.split(\"_\")[0]\n",
    "            patient_images[patient_id].append(os.path.join(directory, filename))\n",
    "\n",
    "    selected_images = []\n",
    "    for patient_id, images in patient_images.items():\n",
    "        for image in images:\n",
    "        #selected_image = random.choice(images) if random_selection else images[0]\n",
    "        \n",
    "            selected_images.append((patient_id, image))\n",
    "            if max_total_images is not None and len(selected_images) >= max_total_images:\n",
    "                break\n",
    "\n",
    "    return selected_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(csv_path, conditions):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    labels = {}\n",
    "    for _, row in df.iterrows():\n",
    "        image_path = row['Image Index']\n",
    "        findings = row['Finding Labels'].split('|')\n",
    "        label_vector = [1 if condition in findings else 0 for condition in conditions]\n",
    "        labels[image_path] = label_vector\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_transforms(image_size=224):\n",
    "    return {\n",
    "        \"train\": transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(7),\n",
    "            #transforms.RandomCrop(image_size, padding=10),\n",
    "            transforms.RandomResizedCrop(\n",
    "                size=(image_size, image_size), \n",
    "                scale=(0.08, 1.0),\n",
    "                ratio=(3/4, 4/3)\n",
    "            ),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "        \"val\": transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def prepare_model(model_name: str, num_classes: int, weights: str = \"DEFAULT\") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Prepare a classification model with custom output classes.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model (must be a valid torchvision model name).\n",
    "        num_classes (int): Number of output classes.\n",
    "        weights (str): Pretrained weights to use. Default is \"DEFAULT\".\n",
    "        \n",
    "    Returns:\n",
    "        model (torch.nn.Module): The prepared model with the custom classification head.\n",
    "    \"\"\"\n",
    "    # Get the model\n",
    "    model = get_model(model_name, weights=weights)\n",
    "\n",
    "    # Replace the classification head based on the model architecture\n",
    "    if hasattr(model, \"fc\"):  # For models like ResNet, RegNet, etc.\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    elif hasattr(model, \"classifier\"):  # For models like DenseNet, VGG, etc.\n",
    "        if isinstance(model.classifier, nn.Linear):\n",
    "            model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "        elif isinstance(model.classifier, nn.Sequential):  # For models like EfficientNet\n",
    "            model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, num_classes)\n",
    "    elif hasattr(model, \"heads\"):  # For Vision Transformers (ViT)\n",
    "        model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} does not have a recognized classification head.\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module,\n",
    "                train_loader: DataLoader,\n",
    "                val_loader: DataLoader,\n",
    "                num_epochs: int,\n",
    "                lr: float,\n",
    "                weight_decay: float,\n",
    "                retrain: bool=True,\n",
    "                grad_clip: float=None,\n",
    "                models_dir: str = \"models\",\n",
    "                checkpoint_interval: int = 5) -> nn.Module:\n",
    "\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    if retrain:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                            lr=lr, weight_decay=weight_decay, betas=(0.9, 0.999), eps=1e-08, amsgrad=False)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
    "    \n",
    "    #optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    epoch_times = []  # Store times for completed epochs\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()  # Start timing the epoch\n",
    "        train_loss, val_loss = 0.0, 0.0\n",
    "        os.makedirs(models_dir, exist_ok=True)  # Ensure models directory exists\n",
    "\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            images, labels = batch['img'].to(device), batch['lab'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast(device_type=\"cuda\", dtype=torch.float16, enabled=use_amp): \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels.float())\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            if grad_clip is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images, labels = batch['img'].to(device), batch['lab'].to(device)\n",
    "                with autocast(device_type=\"cuda\", dtype=torch.float16, enabled=use_amp):\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels.float())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "        \n",
    "\n",
    "        # Calculate epoch duration and remaining time\n",
    "        epoch_duration = time.time() - start_time\n",
    "        epoch_times.append(epoch_duration)\n",
    "        avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
    "        remaining_time = avg_epoch_time * (num_epochs - (epoch + 1))\n",
    "\n",
    "        # Format remaining time as HH:MM:SS\n",
    "        remaining_time_str = time.strftime('%H:%M:%S', time.gmtime(remaining_time))\n",
    "\n",
    "        # Print epoch summary with timing and remaining time\n",
    "        print(f\"    Epoch {epoch+1:03d}/{num_epochs:03d}, \"\n",
    "              f\"Train Loss: {train_losses[-1]:.6f}, \"\n",
    "              f\"Val Loss: {val_losses[-1]:.6f}, \"\n",
    "              f\"Time: {epoch_duration:.2f} sec, \"\n",
    "              f\"ETA: {remaining_time_str}\", end=\" \")\n",
    "        \n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            checkpoint_path = f\"{models_dir}/checkpoint_epoch_{epoch + 1}.pth\"\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(\", Checkpoint saved\")\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model: nn.Module, val_loader: DataLoader, target_names: list) -> dict:\n",
    "    model.eval()\n",
    "\n",
    "    predictions, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images, labels = batch['img'].to(device), batch['lab'].to(device)\n",
    "            outputs = torch.sigmoid(model(images))  # Sigmoid for probabilities\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(labels.cpu().numpy())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "\n",
    "    # Calculate AUC for each label\n",
    "    auc_scores = []\n",
    "    for i in range(len(target_names)):\n",
    "        if np.sum(actuals[:, i]) == 0 or np.sum(actuals[:, i]) == len(actuals):\n",
    "            print(f\"Skipping AUC calculation for {target_names[i]} (only one class present in labels).\")\n",
    "            auc_scores.append(None)\n",
    "        else:\n",
    "            auc = roc_auc_score(actuals[:, i], predictions[:, i])\n",
    "            auc_scores.append(auc)\n",
    "            #print(f\"{target_names[i]}: AUC = {auc:.4f}\")\n",
    "\n",
    "    #auc_score = roc_auc_score(actuals, predictions, average=\"micro\")\n",
    "    #print(f'ROC AUC Score: {auc_score}')\n",
    "\n",
    "    valid_auc_scores = [auc for auc in auc_scores if auc is not None]\n",
    "    if valid_auc_scores:\n",
    "        avg_auc = np.mean(valid_auc_scores)\n",
    "        #print(f\"\\nAverage AUC (excluding skipped labels): {avg_auc:.4f}\")\n",
    "    \n",
    "    return {'predictions': predictions, 'actuals': actuals, 'auc_scores': auc_scores, 'avg_auc': avg_auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def test_model(model: nn.Module, test_loader: DataLoader, target_names: list) -> dict:\n",
    "    model.eval()\n",
    "\n",
    "    predictions, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            images, labels = batch['img'].to(device), batch['lab'].to(device)\n",
    "            outputs = torch.sigmoid(model(images))\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(labels.cpu().numpy())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "\n",
    "    # Calculate AUC for each label\n",
    "    auc_scores = []\n",
    "    for i in range(len(target_names)):\n",
    "        if np.sum(actuals[:, i]) == 0 or np.sum(actuals[:, i]) == len(actuals):\n",
    "            print(f\"Skipping AUC calculation for {target_names[i]} (only one class present in labels).\")\n",
    "            auc_scores.append(None)\n",
    "        else:\n",
    "            auc = roc_auc_score(actuals[:, i], predictions[:, i])\n",
    "            auc_scores.append(auc)\n",
    "            #print(f\"{target_names[i]}: {auc:.4f}\")\n",
    "\n",
    "    valid_auc_scores = [auc for auc in auc_scores if auc is not None]\n",
    "    if valid_auc_scores:\n",
    "        avg_auc = np.mean(valid_auc_scores)\n",
    "        #print(f\"\\nAverage AUC (excluding skipped labels): {avg_auc:.4f}\\n\")\n",
    "    \n",
    "    return {'predictions': predictions, 'actuals': actuals, 'auc_scores': auc_scores, 'avg_auc': avg_auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_combined_radar_chart(results_df):\n",
    "    pathologies = results_df[\"Pathology\"].unique()\n",
    "    num_pathologies = len(pathologies)\n",
    "\n",
    "    # Create angle for each pathology\n",
    "    angles = np.linspace(0, 2 * np.pi, num_pathologies, endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Complete the loop\n",
    "\n",
    "    # Prepare figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    # Draw one axe per pathology and add labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(pathologies, fontsize=10)\n",
    "\n",
    "    # Draw y-labels\n",
    "    ax.set_rscale(\"linear\")\n",
    "    ax.set_rlabel_position(0)\n",
    "    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax.set_yticklabels([\"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], color=\"grey\", size=10)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    # Colors for each model\n",
    "    colors = plt.cm.tab20.colors\n",
    "\n",
    "    # Plot test AUCs for each model\n",
    "    for i, model_name in enumerate(results_df[\"Model\"].unique()):\n",
    "        model_results = results_df[results_df[\"Model\"] == model_name]\n",
    "        avg_auc_per_pathology = model_results.groupby(\"Pathology\")[\"Test AUC\"].mean()\n",
    "\n",
    "        test_aucs = avg_auc_per_pathology.tolist()\n",
    "        test_aucs += test_aucs[:1]  # Complete the loop\n",
    "\n",
    "        ax.plot(angles, test_aucs, label=model_name, linestyle='-', color=colors[i % len(colors)])\n",
    "        ax.fill(angles, test_aucs, color=colors[i % len(colors)], alpha=0.1)\n",
    "\n",
    "    # Add legend and title\n",
    "    plt.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1), fontsize=9)\n",
    "    plt.title(\"Combined Radar Chart for Test AUCs of All Models\", size=15, y=1.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_radar_chart(model_name, results_df, runs_per_model=5):\n",
    "    pathologies = results_df[\"Pathology\"].unique()\n",
    "    num_pathologies = len(pathologies)\n",
    "\n",
    "    # Prepare data for the specified model\n",
    "    model_results = results_df[results_df[\"Model\"] == model_name]\n",
    "    avg_auc_per_pathology = model_results.groupby(\"Pathology\")[[\"Validation AUC\", \"Test AUC\"]].mean()\n",
    "\n",
    "    # Create angle for each pathology\n",
    "    angles = np.linspace(0, 2 * np.pi, num_pathologies, endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Complete the loop\n",
    "\n",
    "    # Prepare data for radar chart\n",
    "    validation_aucs = avg_auc_per_pathology[\"Validation AUC\"].tolist()\n",
    "    test_aucs = avg_auc_per_pathology[\"Test AUC\"].tolist()\n",
    "    validation_aucs += validation_aucs[:1]  # Complete the loop\n",
    "    test_aucs += test_aucs[:1]\n",
    "\n",
    "    # Start the radar plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    # Draw one axe per pathology and add labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(pathologies, fontsize=10)\n",
    "\n",
    "    # Draw y-labels\n",
    "    ax.set_rscale(\"linear\")\n",
    "    ax.set_rlabel_position(0)\n",
    "    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax.set_yticklabels([\"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], color=\"grey\", size=10)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    # Plot data\n",
    "    ax.plot(angles, validation_aucs, label=\"Validation AUC\", linestyle='--', color=\"blue\")\n",
    "    ax.fill(angles, validation_aucs, color=\"blue\", alpha=0.1)\n",
    "\n",
    "    ax.plot(angles, test_aucs, label=\"Test AUC\", linestyle='-', color=\"orange\")\n",
    "    ax.fill(angles, test_aucs, color=\"orange\", alpha=0.1)\n",
    "\n",
    "    # Add legend and title\n",
    "    plt.legend(loc=\"upper right\", bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title(f\"Radar Chart for Model: {model_name}\", size=15, y=1.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_pathologies = [\"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\", \"Effusion\", \n",
    "                        \"Emphysema\", \"Fibrosis\", \"Hernia\", \"Infiltration\", \"Mass\", \n",
    "                        \"Nodule\", \"Pleural_Thickening\", \"Pneumonia\", \"Pneumothorax\"]\n",
    "\n",
    "label_mapping = load_labels(labels_file, common_pathologies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = get_data_transforms() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_subset, val_subset = train_test_split(train_dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataset = load_dataset(train_dir, random_selection=True, seed=42, max_total_images=num_train_images)\n",
    "\n",
    "# Separate IDs and paths\n",
    "ids = [item[0] for item in train_val_dataset]\n",
    "paths = [item[1] for item in train_val_dataset]\n",
    "\n",
    "splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_idx, val_idx in splitter.split(paths, groups=ids):\n",
    "    train_subset = [(ids[i], paths[i]) for i in train_idx]\n",
    "    val_subset = [(ids[i], paths[i]) for i in val_idx]\n",
    "\n",
    "\n",
    "test_dataset = load_dataset(test_dir, random_selection=False, max_total_images=num_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data leakage detected.\n"
     ]
    }
   ],
   "source": [
    "# check for data leakage\n",
    "\n",
    "ids1 = {item[0] for item in train_subset}\n",
    "ids2 = {item[0] for item in val_subset}\n",
    "common_ids = ids1.intersection(ids2)\n",
    "\n",
    "if common_ids:\n",
    "    print(\"Data leakage detected! Common IDs:\", common_ids)\n",
    "else:\n",
    "    print(\"No data leakage detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building cache...\n",
      "Starting worker process 4/4\n",
      "Adding batches to queue: 4352/4352\n",
      "Collecting results: 71/4352, ETA: 00:44:02\r"
     ]
    }
   ],
   "source": [
    "transform_string = \"rhf_rr7_rrc_cj02\"\n",
    "\n",
    "train_dataset = preprocess_and_save(\n",
    "    train_subset,\n",
    "    transform=data_transforms[\"train\"],\n",
    "    cache_dir=f\"train_cache_{transform_string}\",\n",
    "    batch_size=batch_size,\n",
    "    enable_cache=enable_cache,\n",
    "    rebuild_cache=rebuild_cache \n",
    ")\n",
    "\n",
    "val_dataset = preprocess_and_save(\n",
    "    val_subset,\n",
    "    transform=data_transforms[\"val\"],\n",
    "    cache_dir=f\"val_cache_{transform_string}\",\n",
    "    batch_size=batch_size,\n",
    "    enable_cache=enable_cache,\n",
    "    rebuild_cache=rebuild_cache \n",
    ")\n",
    "\n",
    "test_dataset = preprocess_and_save(\n",
    "    test_dataset,\n",
    "    transform=data_transforms[\"val\"],\n",
    "    cache_dir=f\"test_cache_{transform_string}\",\n",
    "    batch_size=batch_size,\n",
    "    enable_cache=enable_cache,\n",
    "    rebuild_cache=rebuild_cache \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    ChestXray14CachedDataset(train_dataset, label_mapping, common_pathologies),\n",
    "    batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, prefetch_factor=prefetch_factor, persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    ChestXray14CachedDataset(val_dataset, label_mapping, common_pathologies),\n",
    "    batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, prefetch_factor=prefetch_factor, persistent_workers=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    ChestXray14CachedDataset(test_dataset, label_mapping, common_pathologies),\n",
    "    batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, prefetch_factor=prefetch_factor, persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_models = list_models(module=torchvision.models)\n",
    "# print(classification_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ScaledDenseNet(nn.Module):\n",
    "    def __init__(self, num_classes=14, pretrained=True):\n",
    "        super(ScaledDenseNet, self).__init__()\n",
    "        # Load pretrained DenseNet121\n",
    "        self.densenet = get_model(\"densenet121\", weights='DEFAULT')\n",
    "\n",
    "        # Modify the initial convolution to handle the larger input size\n",
    "        self.densenet.features.conv0 = nn.Conv2d(\n",
    "            in_channels=3,  # Input channels (RGB)\n",
    "            out_channels=64,  # Same output channels\n",
    "            kernel_size=(7, 7),  # Default kernel size\n",
    "            stride=(2, 2),  # Default stride\n",
    "            padding=(3, 3),  # Default padding\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        # Adjust the classifier for the dataset's number of classes\n",
    "        self.densenet.classifier = nn.Linear(\n",
    "            in_features=self.densenet.classifier.in_features,\n",
    "            out_features=num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.densenet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00013334120505282098\n",
    "weight_decay = 1.8857522141696178e-06\n",
    "grad_clip =  0.47836246526814713\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = prepare_model(\"densenet121\", num_classes=len(common_pathologies), weights=\"DEFAULT\").to(device)\n",
    "# #model = ScaledDenseNet(num_classes=len(common_pathologies), pretrained=True).to(device)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# #optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1.8857522141696178e-06)\n",
    "\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = True\n",
    "# optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), l, weight_decay=, betas=(0.9, 0.999), eps=1e-08, amsgrad=False)\n",
    "# #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience = 5, mode = 'min')\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train_losses, val_losses = [], []\n",
    "# epoch_times = []  # Store times for completed epochs\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     start_time = time.time()  # Start timing the epoch\n",
    "#     train_loss, val_loss = 0.0, 0.0\n",
    "#     os.makedirs(models_dir, exist_ok=True)  # Ensure models directory exists\n",
    "\n",
    "#     # Training Phase\n",
    "#     model.train()\n",
    "#     for batch in train_loader:\n",
    "#         images, labels = batch['img'].to(device), batch['lab'].to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         with autocast(device_type=\"cuda\", dtype=torch.float16, enabled=use_amp): \n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels.float())\n",
    "\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "#     # Validation Phase\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for batch in val_loader:\n",
    "#             images, labels = batch['img'].to(device), batch['lab'].to(device)\n",
    "#             with autocast(device_type=\"cuda\", dtype=torch.float16, enabled=use_amp):\n",
    "#                 outputs = model(images)\n",
    "#                 loss = criterion(outputs, labels.float())\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#     train_losses.append(train_loss / len(train_loader))\n",
    "#     val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "#     # Calculate epoch duration and remaining time\n",
    "#     epoch_duration = time.time() - start_time\n",
    "#     epoch_times.append(epoch_duration)\n",
    "#     avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
    "#     remaining_time = avg_epoch_time * (num_epochs - (epoch + 1))\n",
    "\n",
    "#     # Format remaining time as HH:MM:SS\n",
    "#     remaining_time_str = time.strftime('%H:%M:%S', time.gmtime(remaining_time))\n",
    "\n",
    "#     # Print epoch summary with timing and remaining time\n",
    "#     print(f\"    Epoch {epoch+1:03d}/{num_epochs:03d}, \"\n",
    "#             f\"Train Loss: {train_losses[-1]:.6f}, \"\n",
    "#             f\"Val Loss: {val_losses[-1]:.6f}, \"\n",
    "#             f\"Time: {epoch_duration:.2f} sec, \"\n",
    "#             f\"ETA: {remaining_time_str}\", end=\" \")\n",
    "    \n",
    "#     if (epoch + 1) % checkpoint_interval == 0:\n",
    "#         checkpoint_path = f\"{models_dir}/checkpoint_epoch_{epoch + 1}.pth\"\n",
    "#         torch.save(model.state_dict(), checkpoint_path)\n",
    "#         print(\", Checkpoint saved\")\n",
    "#     else:\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Evaluate and test the model\n",
    "# results_eval = evaluate_model(model, val_loader, target_names=common_pathologies)\n",
    "# results_test = test_model(model, test_loader, target_names=common_pathologies)\n",
    "\n",
    "# # Collect results\n",
    "# new_results = []\n",
    "# for i, pathology in enumerate(common_pathologies):\n",
    "#     val_auc = results_eval['auc_scores'][i] if results_eval['auc_scores'][i] is not None else np.nan\n",
    "#     test_auc = results_test['auc_scores'][i] if results_test['auc_scores'][i] is not None else np.nan\n",
    "\n",
    "#     new_results.append({\n",
    "#         \"Model\": model,\n",
    "#         \"Run\": 1,\n",
    "#         \"Pathology\": pathology,\n",
    "#         \"Validation AUC\": val_auc,\n",
    "#         \"Test AUC\": test_auc\n",
    "#     })\n",
    "\n",
    "# new_results_df = pd.DataFrame(new_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# new_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing results\n",
    "if os.path.exists(detailed_results_path):\n",
    "    results_df = pd.read_csv(detailed_results_path)\n",
    "    completed_runs = set((row[\"Model\"], row[\"Run\"]) for _, row in results_df.iterrows())\n",
    "else:\n",
    "    results_df = pd.DataFrame(columns=[\"Model\", \"Run\", \"Pathology\", \"Validation AUC\", \"Test AUC\"])\n",
    "    completed_runs = set()\n",
    "\n",
    "# Specify models to test\n",
    "\n",
    "\n",
    "# Iterate over models\n",
    "for model_name in model_names:\n",
    "    print(f\"Running experiments for model: {model_name}\")\n",
    "    \n",
    "    for run in range(1, runs_per_model + 1):\n",
    "        run_identifier = (model_name, run)\n",
    "        run_lock_path = os.path.join(locks_dir, f\"{model_name}_run_{run}.lock\")\n",
    "        run_lock = FileLock(run_lock_path)\n",
    "\n",
    "        # Check for existing lock\n",
    "        if os.path.exists(run_lock_path):\n",
    "            lock_age = time.time() - os.path.getmtime(run_lock_path)\n",
    "            if lock_age < lock_timeout:\n",
    "                print(f\"  Skipping locked run {run} for model {model_name} (lock age: {lock_age:.1f} seconds)\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"  Found stale lock for run {run} of model {model_name}. Removing...\")\n",
    "                os.remove(run_lock_path)\n",
    "            \n",
    "        # Create a lock atomically\n",
    "        try:\n",
    "            with open(run_lock_path, \"x\") as lock_file:\n",
    "                lock_file.write(str(time.time()))  # Write timestamp to the lock file\n",
    "        except FileExistsError:\n",
    "            print(f\"  Skipping locked run {run} for model {model_name} (race condition)\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"  Starting run {run} for model {model_name}\")\n",
    "\n",
    "            # Prepare and train the model\n",
    "            model = prepare_model(model_name=model_name, num_classes=len(common_pathologies), weights=\"DEFAULT\")\n",
    "            model = train_model(model,\n",
    "                                train_loader,\n",
    "                                val_loader,\n",
    "                                num_epochs,\n",
    "                                lr,\n",
    "                                weight_decay,\n",
    "                                retrain=True,\n",
    "                                grad_clip=grad_clip,\n",
    "                                checkpoint_interval=checkpoint_interval)\n",
    "\n",
    "            # Evaluate and test the model\n",
    "            results_eval = evaluate_model(model, val_loader, target_names=common_pathologies)\n",
    "            results_test = test_model(model, test_loader, target_names=common_pathologies)\n",
    "\n",
    "            # Collect results\n",
    "            new_results = []\n",
    "            for i, pathology in enumerate(common_pathologies):\n",
    "                val_auc = results_eval['auc_scores'][i] if results_eval['auc_scores'][i] is not None else np.nan\n",
    "                test_auc = results_test['auc_scores'][i] if results_test['auc_scores'][i] is not None else np.nan\n",
    "\n",
    "                new_results.append({\n",
    "                    \"Model\": model_name,\n",
    "                    \"Run\": run,\n",
    "                    \"Pathology\": pathology,\n",
    "                    \"Validation AUC\": val_auc,\n",
    "                    \"Test AUC\": test_auc\n",
    "                })\n",
    "\n",
    "            # Append results safely with a global file lock\n",
    "            file_lock_path = detailed_results_path + \".lock\"\n",
    "            with FileLock(file_lock_path):\n",
    "                new_results_df = pd.DataFrame(new_results)\n",
    "                if os.path.exists(detailed_results_path):\n",
    "                    new_results_df.to_csv(detailed_results_path, mode='a', header=False, index=False)\n",
    "                else:\n",
    "                    new_results_df.to_csv(detailed_results_path, index=False)\n",
    "\n",
    "            print(f\"  Results saved for model {model_name}, run {run}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during run {run} for model {model_name}: {e}\")\n",
    "\n",
    "        finally:\n",
    "            # Remove the run lock\n",
    "            # if os.path.exists(run_lock_path):\n",
    "            #     os.remove(run_lock_path)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Generate summary statistics after all runs\n",
    "with FileLock(detailed_results_path + \".lock\"):\n",
    "    results_df = pd.read_csv(detailed_results_path)\n",
    "\n",
    "results_df[\"Validation AUC\"] = pd.to_numeric(results_df[\"Validation AUC\"], errors=\"coerce\")\n",
    "results_df[\"Test AUC\"] = pd.to_numeric(results_df[\"Test AUC\"], errors=\"coerce\")\n",
    "\n",
    "summary = results_df.groupby([\"Model\", \"Pathology\"]).agg({\n",
    "    \"Validation AUC\": [\"mean\", \"std\"],\n",
    "    \"Test AUC\": [\"mean\", \"std\"]\n",
    "}).reset_index()\n",
    "\n",
    "summary.to_csv(summary_results_path, index=False)\n",
    "\n",
    "print(\"Summary of Results:\")\n",
    "from IPython.display import display\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = f\"{models_dir}/model_{timestamp}.pth\"\n",
    "torch.save(model.state_dict(), model_filename)\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(detailed_results_path)\n",
    "\n",
    "plot_combined_radar_chart(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate radar chart for each model\n",
    "for model_name in results_df[\"Model\"].unique():\n",
    "    plot_radar_chart(model_name, results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
