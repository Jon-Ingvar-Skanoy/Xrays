{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchxrayvision matplotlib numpy pandas ipywidgets scikit-learn scikit-image seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TorchXRayVision: A library of chest X-ray datasets and models](https://arxiv.org/pdf/2111.00595)\n",
    "\n",
    "https://github.com/naitik2314/Chest-X-Ray-Medical-Diagnosis-with-Deep-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General utilities\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# PyTorch and data handling\n",
    "import torch, torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Image processing\n",
    "# import skimage\n",
    "# import skimage.transform\n",
    "\n",
    "# Machine learning metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    balanced_accuracy_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    ")\n",
    "\n",
    "# TorchXRayVision library\n",
    "# import torchxrayvision as xrv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = max(1, os.cpu_count() // 2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# use_amp = True\n",
    "# scaler = torch.amp.GradScaler(enabled=use_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_samples = 1000\n",
    "num_epochs = 5\n",
    "unique_patients = True\n",
    "pathology_masks = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine: DESKTOP-UHDJ875, User: jonal, OS: Linux\n"
     ]
    }
   ],
   "source": [
    "machine_name = platform.node()  \n",
    "user = os.getenv(\"USER\") or os.getenv(\"USERNAME\") \n",
    "os_name = platform.system()  # Get os\n",
    "print(f\"Machine: {machine_name}, User: {user}, OS: {os_name}\")\n",
    "\n",
    "if machine_name == \"Corsair\" and os_name == \"Linux\" and user == \"jon\":\n",
    "    windows_drive = Path(\"/mnt/b/Xray\")\n",
    "    paths = {\n",
    "        \"dataset\": windows_drive / \"dataset\",\n",
    "        \"tar_images\": windows_drive / \"dataset/images\",\n",
    "        \"images\": windows_drive / \"dataset/images/images\",\n",
    "        \"checkpoints\": windows_drive / \"checkpoints\",\n",
    "        \"papers\": windows_drive / \"papers\",\n",
    "        \"models\": windows_drive / \"models\",\n",
    "    }\n",
    "\n",
    "    batch_size = 64\n",
    "\n",
    "else:\n",
    "    dataset_dir = Path(\"dataset\")\n",
    "    paths = {\n",
    "        \"dataset\": dataset_dir,\n",
    "        \"tar_images\": dataset_dir / \"images\",\n",
    "        \"images\": dataset_dir / \"images/images\",\n",
    "        \"checkpoints\": dataset_dir / \"checkpoints\",\n",
    "        \"papers\": dataset_dir / \"papers\",\n",
    "        \"models\": dataset_dir / \"models\",\n",
    "    }\n",
    "\n",
    "for key, path in paths.items():\n",
    "    path.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dir_tar_images = paths['tar_images']\n",
    "\n",
    "path_dataset = paths['dataset']\n",
    "path_images = paths['images']\n",
    "path_csv_list = paths['dataset'] / \"Data_Entry_2017_v2020.csv\"\n",
    "path_train_val_list = paths['dataset'] / \"train_val_list.txt\"\n",
    "path_test_list = paths['dataset'] / \"test_list.txt\"\n",
    "path_models = paths['models']\n",
    "checkpoint_dir = paths['checkpoints']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def download_file(link, folder, idx):\n",
    "    \"\"\"Downloads a file from a link to the specified folder.\"\"\"\n",
    "    file_name = f'images_{idx+1:03d}.tar.gz'\n",
    "    file_path = os.path.join(folder, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        #print(f\"{file_name} already exists, skipping download.\")\n",
    "        return file_path\n",
    "    try:\n",
    "        print(f\"Downloading {file_name}...\")\n",
    "        urllib.request.urlretrieve(link, file_path)\n",
    "        print(f\"{file_name} downloaded successfully.\")\n",
    "        return file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {file_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_file(file_path, folder):\n",
    "    \"\"\"Extracts a .tar.gz file to the specified folder.\"\"\"\n",
    "    extracted_flag = file_path.replace('.tar.gz', '_extracted.flag')\n",
    "    if os.path.exists(extracted_flag):\n",
    "        #print(f\"{os.path.basename(file_path)} already extracted, skipping.\")\n",
    "        return\n",
    "    try:\n",
    "        print(f\"Extracting {os.path.basename(file_path)}...\")\n",
    "        with tarfile.open(file_path, 'r:gz') as tar:\n",
    "            tar.extractall(path=folder)\n",
    "        with open(extracted_flag, 'w') as f:\n",
    "            f.write('extracted')\n",
    "        print(f\"{os.path.basename(file_path)} extracted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract {os.path.basename(file_path)}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def process_link(idx, link):\n",
    "    \"\"\"Handles downloading and extracting a single link.\"\"\"\n",
    "    file_path = download_file(link, dir_tar_images, idx)\n",
    "    if file_path:\n",
    "        extract_file(file_path, dir_tar_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_file_lists(train_val_path, test_path):\n",
    "    \"\"\"\n",
    "    Load file lists from the provided paths.\n",
    "    \"\"\"\n",
    "    with open(train_val_path, 'r') as f:\n",
    "        train_val_files = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "    with open(test_path, 'r') as f:\n",
    "        test_files = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    return train_val_files, test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def split_dataset_by_file_list(dataset, train_val_files, test_files):\n",
    "    \"\"\"\n",
    "    Split the dataset into train/validation and test sets based on file lists.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The full NIH_Dataset object.\n",
    "        train_val_files: List of filenames for train/validation.\n",
    "        test_files: List of filenames for test.\n",
    "    \n",
    "    Returns:\n",
    "        train_val_dataset, test_dataset: Subsets of the original dataset.\n",
    "    \"\"\"\n",
    "    # Create a mask for matching filenames\n",
    "    train_val_mask = dataset.csv['Image Index'].isin(train_val_files)\n",
    "    test_mask = dataset.csv['Image Index'].isin(test_files)\n",
    "\n",
    "    # Get indices of matched files\n",
    "    train_val_indices = dataset.csv[train_val_mask].index.tolist()\n",
    "    test_indices = dataset.csv[test_mask].index.tolist()\n",
    "\n",
    "    # Create dataset subsets\n",
    "    train_val_dataset = Subset(dataset, train_val_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "    return train_val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_path, model, optimizer=None, scaler=None):\n",
    "    \"\"\"\n",
    "    Loads a checkpoint and restores the model, optimizer, and scaler states.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path: Path to the checkpoint file.\n",
    "        model: The model to restore.\n",
    "        optimizer: The optimizer to restore (optional).\n",
    "        scaler: The gradient scaler to restore (optional).\n",
    "\n",
    "    Returns:\n",
    "        start_epoch: The epoch to continue training from.\n",
    "        checkpoint: The full checkpoint dictionary.\n",
    "    \"\"\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        if optimizer:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        if scaler:\n",
    "            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
    "        return start_epoch, checkpoint\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {checkpoint_path}. Starting from scratch.\")\n",
    "        return 0, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_samples_with_masks(samples, dataset, max_samples=4):\n",
    "    \"\"\"\n",
    "    Plot multiple X-ray images side-by-side along with their semantic and pathology masks (if available).\n",
    "    Only plots samples with available masks, up to `max_samples`.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for sample in samples:\n",
    "        if \"pathology_masks\" in sample and sample[\"pathology_masks\"]:\n",
    "            width = len(sample[\"pathology_masks\"])\n",
    "            fig, axs = plt.subplots(1, max(2, 1 + width), sharey=True, figsize=(3 + 3 * width, 3))\n",
    "            axs[0].imshow(sample[\"img\"][0], cmap=\"Greys_r\")\n",
    "            axs[0].set_title(f\"Index: {sample['idx']}\")\n",
    "            for i, patho in enumerate(sample[\"pathology_masks\"].keys()):\n",
    "                axs[i + 1].imshow(sample[\"img\"][0], cmap=\"Greys_r\")\n",
    "                axs[i + 1].imshow(sample[\"pathology_masks\"][patho][0] + 1, alpha=0.5)\n",
    "                axs[i + 1].set_title(dataset.pathologies[patho])\n",
    "            plt.show()\n",
    "            count += 1\n",
    "\n",
    "        elif \"semantic_masks\" in sample and sample[\"semantic_masks\"]:\n",
    "            width = len(sample[\"semantic_masks\"])\n",
    "            fig, axs = plt.subplots(1, max(2, 1 + width), sharey=True, figsize=(3 + 3 * width, 3))\n",
    "            axs[0].imshow(sample[\"img\"][0], cmap=\"Greys_r\")\n",
    "            axs[0].set_title(f\"Index: {sample['idx']}\")\n",
    "            for i, patho in enumerate(sample[\"semantic_masks\"].keys()):\n",
    "                axs[i + 1].imshow(sample[\"img\"][0], cmap=\"Greys_r\")\n",
    "                axs[i + 1].imshow(sample[\"semantic_masks\"][patho][0] + 1, alpha=0.5)\n",
    "                axs[i + 1].set_title(patho)\n",
    "            plt.show()\n",
    "            count += 1\n",
    "\n",
    "        if count >= max_samples:  # Stop after plotting max_samples\n",
    "            break\n",
    "\n",
    "    if count == 0:\n",
    "        print(\"No samples with masks found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images_001.tar.gz...\n",
      "Downloading images_002.tar.gz...\n",
      "Downloading images_003.tar.gz...\n",
      "Downloading images_004.tar.gz...\n",
      "Downloading images_005.tar.gz...\n",
      "Downloading images_006.tar.gz...\n",
      "Downloading images_007.tar.gz...\n",
      "Downloading images_008.tar.gz...\n",
      "images_001.tar.gz downloaded successfully.\n",
      "Extracting images_001.tar.gz...\n",
      "images_004.tar.gz downloaded successfully.\n",
      "Extracting images_004.tar.gz...\n",
      "images_008.tar.gz downloaded successfully.\n",
      "Extracting images_008.tar.gz...\n",
      "images_007.tar.gz downloaded successfully.\n",
      "Extracting images_007.tar.gz...\n",
      "images_002.tar.gz downloaded successfully.\n",
      "Extracting images_002.tar.gz...\n",
      "images_006.tar.gz downloaded successfully.\n",
      "Extracting images_006.tar.gz...\n",
      "images_005.tar.gz downloaded successfully.\n",
      "Extracting images_005.tar.gz...\n",
      "images_003.tar.gz downloaded successfully.\n",
      "Extracting images_003.tar.gz...\n",
      "images_001.tar.gz extracted successfully.\n",
      "Downloading images_009.tar.gz...\n",
      "images_009.tar.gz downloaded successfully.\n",
      "Extracting images_009.tar.gz...\n",
      "images_004.tar.gz extracted successfully.\n",
      "Downloading images_010.tar.gz...\n",
      "images_002.tar.gz extracted successfully.\n",
      "Downloading images_011.tar.gz...\n",
      "images_008.tar.gz extracted successfully.\n",
      "Downloading images_012.tar.gz...\n",
      "images_007.tar.gz extracted successfully.\n",
      "images_005.tar.gz extracted successfully.\n",
      "images_006.tar.gz extracted successfully.\n",
      "images_003.tar.gz extracted successfully.\n",
      "images_012.tar.gz downloaded successfully.\n",
      "Extracting images_012.tar.gz...\n",
      "images_010.tar.gz downloaded successfully.\n",
      "Extracting images_010.tar.gz...\n",
      "images_011.tar.gz downloaded successfully.\n",
      "Extracting images_011.tar.gz...\n",
      "images_009.tar.gz extracted successfully.\n",
      "images_012.tar.gz extracted successfully.\n",
      "images_010.tar.gz extracted successfully.\n",
      "images_011.tar.gz extracted successfully.\n",
      "Download and extraction complete. Please check the extracted files.\n"
     ]
    }
   ],
   "source": [
    "links = [\n",
    "    'https://nihcc.box.com/shared/static/vfk49d74nhbxq3nqjg0900w5nvkorp5c.gz',\n",
    "    'https://nihcc.box.com/shared/static/i28rlmbvmfjbl8p2n3ril0pptcmcu9d1.gz',\n",
    "    'https://nihcc.box.com/shared/static/f1t00wrtdk94satdfb9olcolqx20z2jp.gz',\n",
    "\t'https://nihcc.box.com/shared/static/0aowwzs5lhjrceb3qp67ahp0rd1l1etg.gz',\n",
    "    'https://nihcc.box.com/shared/static/v5e3goj22zr6h8tzualxfsqlqaygfbsn.gz',\n",
    "\t'https://nihcc.box.com/shared/static/asi7ikud9jwnkrnkj99jnpfkjdes7l6l.gz',\n",
    "\t'https://nihcc.box.com/shared/static/jn1b4mw4n6lnh74ovmcjb8y48h8xj07n.gz',\n",
    "    'https://nihcc.box.com/shared/static/tvpxmn7qyrgl0w8wfh9kqfjskv6nmm1j.gz',\n",
    "\t'https://nihcc.box.com/shared/static/upyy3ml7qdumlgk2rfcvlb9k6gvqq2pj.gz',\n",
    "\t'https://nihcc.box.com/shared/static/l6nilvfa9cg3s28tqv1qc1olm3gnz54p.gz',\n",
    "\t'https://nihcc.box.com/shared/static/hhq8fkdgvcari67vfhs7ppg2w6ni4jze.gz',\n",
    "\t'https://nihcc.box.com/shared/static/ioqwiy20ihqwyr8pf4c24eazhh281pbu.gz'\n",
    "]\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    executor.map(lambda args: process_link(*args), enumerate(links))\n",
    "\n",
    "print(\"Download and extraction complete. Please check the extracted files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([\n",
    "    xrv.datasets.XRayCenterCrop(),   # Crop the center of the image\n",
    "    xrv.datasets.XRayResizer(224),  # Resize to 224x224\n",
    "    # transforms.ToTensor(),          # Convert to tensor\n",
    "    # transforms.Lambda(lambda x: x.unsqueeze(0) if x.dim() == 2 else x),  # Add channel dimension if missing\n",
    "    # transforms.Lambda(lambda x: (x / 2048.0) * 1024.0)  # Normalize to [-1024, 1024]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lung Lesion doesn't exist. Adding nans instead.\n",
      "Fracture doesn't exist. Adding nans instead.\n",
      "Lung Opacity doesn't exist. Adding nans instead.\n",
      "Enlarged Cardiomediastinum doesn't exist. Adding nans instead.\n"
     ]
    }
   ],
   "source": [
    "dataset = xrv.datasets.NIH_Dataset(\n",
    "    imgpath=path_images,\n",
    "    csvpath=f\"{path_dataset}/Data_Entry_2017_v2020.csv\",\n",
    "    transform=transforms,\n",
    "    views=[\"PA\", \"AP\"],\n",
    "    unique_patients=unique_patients, # One image per patient\n",
    "    pathology_masks=pathology_masks, # Load pathology masks\n",
    ")\n",
    "xrv.datasets.relabel_dataset(xrv.datasets.default_pathologies, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Atelectasis': {0.0: 29140, 1.0: 1665},\n",
      " 'Cardiomegaly': {0.0: 30028, 1.0: 777},\n",
      " 'Consolidation': {0.0: 30369, 1.0: 436},\n",
      " 'Edema': {0.0: 30727, 1.0: 78},\n",
      " 'Effusion': {0.0: 29565, 1.0: 1240},\n",
      " 'Emphysema': {0.0: 30539, 1.0: 266},\n",
      " 'Enlarged Cardiomediastinum': {},\n",
      " 'Fibrosis': {0.0: 30239, 1.0: 566},\n",
      " 'Fracture': {},\n",
      " 'Hernia': {0.0: 30724, 1.0: 81},\n",
      " 'Infiltration': {0.0: 27354, 1.0: 3451},\n",
      " 'Lung Lesion': {},\n",
      " 'Lung Opacity': {},\n",
      " 'Mass': {0.0: 29521, 1.0: 1284},\n",
      " 'Nodule': {0.0: 29135, 1.0: 1670},\n",
      " 'Pleural_Thickening': {0.0: 30051, 1.0: 754},\n",
      " 'Pneumonia': {0.0: 30631, 1.0: 174},\n",
      " 'Pneumothorax': {0.0: 30553, 1.0: 252}}\n",
      "NIH_Dataset num_samples=30805 views=['PA', 'AP'] data_aug=None\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val Dataset: 28008 samples\n",
      "Test Dataset: 2797 samples\n"
     ]
    }
   ],
   "source": [
    "train_val_files, test_files = load_file_lists(path_train_val_list, path_test_list)\n",
    "train_val_dataset, test_dataset = split_dataset_by_file_list(dataset, train_val_files, test_files)\n",
    "\n",
    "print(f\"Train/Val Dataset: {len(train_val_dataset)} samples\")\n",
    "print(f\"Test Dataset: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 800 samples\n",
      "Validation Dataset: 200 samples\n"
     ]
    }
   ],
   "source": [
    "indices = list(range(num_samples))\n",
    "\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = Subset(train_val_dataset, train_indices)\n",
    "val_dataset = Subset(train_val_dataset, val_indices)\n",
    "\n",
    "print(f\"Train Dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Validation Dataset: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=num_workers\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=num_workers\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_loader:\n",
    "#     images, labels = batch['img'], batch['lab']\n",
    "#     print(f\"Train batch: {images.shape}, {labels.shape}\")\n",
    "#     break\n",
    "\n",
    "# for batch in val_loader:\n",
    "#     images, labels = batch['img'], batch['lab']\n",
    "#     print(f\"Validation batch: {images.shape}, {labels.shape}\")\n",
    "#     break\n",
    "\n",
    "# for batch in test_loader:\n",
    "#     images, labels = batch['img'], batch['lab']\n",
    "#     print(f\"Test batch: {images.shape}, {labels.shape}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading weights...\n",
      "If this fails you can run `wget https://github.com/mlmed/torchxrayvision/releases/download/v1/nih-densenet121-d121-tw-lr001-rot45-tr15-sc15-seed0-best.pt -O C:\\Users\\jonin\\.torchxrayvision\\models_data/nih-densenet121-d121-tw-lr001-rot45-tr15-sc15-seed0-best.pt`\n",
      "[██████████████████████████████████████████████████]\n"
     ]
    }
   ],
   "source": [
    "model = xrv.models.DenseNet(weights=\"nih\").to(device)\n",
    "model.op_threshs = None # Disable calibrated thresholds for the model\n",
    "\n",
    "# dict(zip(model.pathologies,xrv.datasets.default_pathologies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Pathologies: ['Atelectasis', 'Effusion', 'Infiltration', 'Consolidation', 'Nodule', 'Edema', 'Pneumonia', 'Fibrosis', 'Cardiomegaly', 'Emphysema', 'Mass', 'Pleural_Thickening', 'Hernia', 'Pneumothorax']\n",
      "Dataset to Model Index Mapping: {0: 0, 7: 7, 2: 2, 1: 1, 11: 11, 4: 4, 8: 8, 6: 6, 10: 10, 5: 5, 12: 12, 9: 9, 13: 13, 3: 3}\n"
     ]
    }
   ],
   "source": [
    "# Align dataset pathologies to model pathologies\n",
    "common_pathologies = list(set(dataset.pathologies) & set(model.pathologies))\n",
    "num_common_pathologies = len(common_pathologies)\n",
    "print(f\"Common Pathologies: {common_pathologies}\")\n",
    "\n",
    "# Map dataset indices to model indices\n",
    "dataset_to_model_indices = {dataset.pathologies.index(p): model.pathologies.index(p) for p in common_pathologies}\n",
    "print(f\"Dataset to Model Index Mapping: {dataset_to_model_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated classifier to output 14 pathologies.\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of output features dynamically\n",
    "dummy_input = torch.zeros(1, 1, 224, 224)  # Batch size 1, single channel, 224x224 input\n",
    "if torch.cuda.is_available():\n",
    "    dummy_input = dummy_input.cuda()\n",
    "\n",
    "# Get the output shape of the feature extractor\n",
    "with torch.no_grad():\n",
    "    num_features = model.features(dummy_input).shape[1]  # The second dimension is the feature size\n",
    "\n",
    "# Update the classifier to match the number of pathologies\n",
    "model.classifier = torch.nn.Linear(num_features, num_common_pathologies).to(device)\n",
    "print(f\"Updated classifier to output {num_common_pathologies} pathologies.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated classifier to output 14 pathologies.\n"
     ]
    }
   ],
   "source": [
    "# Update classifier to match the number of common pathologies\n",
    "model.classifier = torch.nn.Linear(num_features, num_common_pathologies).to(device)\n",
    "print(f\"Updated classifier to output {num_common_pathologies} pathologies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.classifier.parameters()) # only train classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\torch\\utils\\data\\dataset.py\", line 418, in __getitems__\n    return self.dataset.__getitems__([self.indices[idx] for idx in indices])  # type: ignore[attr-defined]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\torch\\utils\\data\\dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n            ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\torchxrayvision\\datasets.py\", line 508, in __getitem__\n    img = imread(img_path)\n          ^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\skimage\\io\\_io.py\", line 60, in imread\n    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\skimage\\io\\manage_plugins.py\", line 217, in call_plugin\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\skimage\\io\\_plugins\\imageio_plugin.py\", line 11, in imread\n    out = np.asarray(imageio_imread(*args, **kwargs))\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\imageio\\v3.py\", line 53, in imread\n    with imopen(uri, \"r\", **plugin_kwargs) as img_file:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\imageio\\core\\imopen.py\", line 113, in imopen\n    request = Request(uri, io_mode, format_hint=format_hint, extension=extension)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\imageio\\core\\request.py\", line 247, in __init__\n    self._parse_uri(uri)\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\imageio\\core\\request.py\", line 407, in _parse_uri\n    raise FileNotFoundError(\"No such file: '%s'\" % fn)\nFileNotFoundError: No such file: 'c:\\Users\\jonin\\Documents\\IKT450\\Xrays\\dataset\\images\\images\\00000085_000.png'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     22\u001b[0m epoch_train_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 23\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Prepare data\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_tensors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1465\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1464\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1491\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\torch\\_utils.py:715\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 715\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\torch\\utils\\data\\dataset.py\", line 418, in __getitems__\n    return self.dataset.__getitems__([self.indices[idx] for idx in indices])  # type: ignore[attr-defined]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\torch\\utils\\data\\dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n            ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\torchxrayvision\\datasets.py\", line 508, in __getitem__\n    img = imread(img_path)\n          ^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\skimage\\io\\_io.py\", line 60, in imread\n    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\skimage\\io\\manage_plugins.py\", line 217, in call_plugin\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\skimage\\io\\_plugins\\imageio_plugin.py\", line 11, in imread\n    out = np.asarray(imageio_imread(*args, **kwargs))\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\imageio\\v3.py\", line 53, in imread\n    with imopen(uri, \"r\", **plugin_kwargs) as img_file:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\imageio\\core\\imopen.py\", line 113, in imopen\n    request = Request(uri, io_mode, format_hint=format_hint, extension=extension)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\imageio\\core\\request.py\", line 247, in __init__\n    self._parse_uri(uri)\n  File \"c:\\Users\\jonin\\.conda\\envs\\pytorch_24\\Lib\\site-packages\\imageio\\core\\request.py\", line 407, in _parse_uri\n    raise FileNotFoundError(\"No such file: '%s'\" % fn)\nFileNotFoundError: No such file: 'c:\\Users\\jonin\\Documents\\IKT450\\Xrays\\dataset\\images\\images\\00000085_000.png'\n"
     ]
    }
   ],
   "source": [
    "model_name = \"first_model\"\n",
    "checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_{model_name}.pth\")\n",
    "\n",
    "# Load checkpoint\n",
    "if os.path.exists(checkpoint_path):\n",
    "    start_epoch, checkpoint = load_checkpoint(checkpoint_path, model, optimizer, scaler)\n",
    "    if checkpoint:\n",
    "        train_losses = checkpoint.get('train_losses', [])\n",
    "        val_losses = checkpoint.get('val_losses', [])\n",
    "        all_results = checkpoint.get('all_results', [])\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    all_results = []\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    # ========================\n",
    "    # Training\n",
    "    # ========================\n",
    "    model.train()\n",
    "    epoch_train_losses = []\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # Prepare data\n",
    "        batch_tensors = {k: v.to(device, non_blocking=True) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Extract targets and align for the model\n",
    "        dataset_indices = list(dataset_to_model_indices.keys())\n",
    "        model_indices = list(dataset_to_model_indices.values())\n",
    "        targets = batch_tensors[\"lab\"][:, dataset_indices].float().to(device)\n",
    "        targets_aligned = torch.zeros((targets.size(0), len(model.pathologies)), device=device, dtype=torch.float)\n",
    "        targets_aligned[:, model_indices] = targets\n",
    "\n",
    "        # Forward pass with mixed precision\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
    "            img_input = batch_tensors[\"img\"].to(torch.float16)  # Convert to HalfTensor\n",
    "            outputs = model(img_input)\n",
    "            loss = criterion(outputs[:, model_indices], targets_aligned[:, model_indices])\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Log training loss\n",
    "        epoch_train_losses.append(loss.item())\n",
    "\n",
    "    # Store average training loss for this epoch\n",
    "    avg_train_loss = np.mean(epoch_train_losses)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # ========================\n",
    "    # Validation\n",
    "    # ========================\n",
    "    model.eval()\n",
    "    outs, labs = [], []\n",
    "    results = []  # Initialize for each epoch\n",
    "    epoch_val_losses = []  # To store validation losses for the epoch\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # Prepare validation data\n",
    "            batch_tensors = {k: v.to(device, non_blocking=True) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "            targets = batch_tensors[\"lab\"][:, dataset_indices].float().to(device)\n",
    "            targets_aligned = torch.zeros((targets.size(0), len(model.pathologies)), device=device, dtype=torch.float)\n",
    "            targets_aligned[:, model_indices] = targets\n",
    "\n",
    "            # Forward pass\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
    "                img_input = batch_tensors[\"img\"]\n",
    "                outputs = model(img_input)\n",
    "                loss = criterion(outputs[:, model_indices], targets_aligned[:, model_indices])\n",
    "\n",
    "            # Log validation loss for the batch\n",
    "            epoch_val_losses.append(loss.item())\n",
    "\n",
    "            # Log outputs and labels\n",
    "            outs.extend(outputs[:, model_indices].detach().cpu().numpy())\n",
    "            labs.extend(targets_aligned[:, model_indices].detach().cpu().numpy())\n",
    "\n",
    "\n",
    "        # Calculate Metrics\n",
    "        outs = np.array(outs)\n",
    "        labs = np.array(labs)\n",
    "        for idx, pathology in enumerate(common_pathologies):\n",
    "            result = {\"Pathology\": pathology}\n",
    "            if len(np.unique(labs[:, idx])) > 1:  # Only calculate metrics if labels are not constant\n",
    "                labels = labs[:, idx].astype(bool)\n",
    "                preds = outs[:, idx]\n",
    "                result[\"AUC\"] = roc_auc_score(labels, preds)\n",
    "                result[\"Acc\"] = accuracy_score(labels, preds > 0.5)\n",
    "                result[\"F1\"] = f1_score(labels, preds > 0.5)\n",
    "                result[\"Precision\"] = precision_score(labels, preds > 0.5)\n",
    "                result[\"Recall\"] = recall_score(labels, preds > 0.5)\n",
    "                tn, fp, fn, tp = confusion_matrix(labels, preds > 0.5).ravel()\n",
    "                result[\"Specificity\"] = tn / (tn + fp)\n",
    "                result[\"Balanced Accuracy\"] = balanced_accuracy_score(labels, preds > 0.5)\n",
    "                precision, recall, _ = precision_recall_curve(labels, preds)\n",
    "                result[\"PR AUC\"] = auc(recall, precision)\n",
    "            results.append(result)\n",
    "        \n",
    "        # Append to all_results\n",
    "        if len(results) > 0:\n",
    "            df_results = pd.DataFrame(results)\n",
    "            all_results.append(df_results)\n",
    "            #print(f\"Epoch {epoch + 1} Evaluation Results:\")\n",
    "            #display(df_results)\n",
    "        else:\n",
    "            print(f\"No metrics calculated for epoch {epoch + 1}.\")\n",
    "    \n",
    "    # Store average validation loss for the epoch\n",
    "    avg_val_loss = np.mean(epoch_val_losses)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} - Training Loss: {avg_train_loss:.4f}, Validation Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # ========================\n",
    "    # Log and Save Checkpoints\n",
    "    # ========================\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scaler_state_dict': scaler.state_dict(),\n",
    "        'train_losses': train_losses,  # Save loss history\n",
    "        'val_losses': val_losses,\n",
    "        'all_results': all_results,   # Save all results\n",
    "    }, checkpoint_path)\n",
    "\n",
    "# ========================\n",
    "# Loss Curve\n",
    "# ========================\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# ========================\n",
    "# Final Results\n",
    "# ========================\n",
    "if len(all_results) > 0:\n",
    "    final_results = pd.concat(all_results, keys=range(1, len(all_results) + 1))\n",
    "    final_results.to_csv(os.path.join(checkpoint_dir, \"evaluation_results.csv\"), index=False)\n",
    "    print(\"Evaluation results saved.\")\n",
    "\n",
    "    # Summary of metrics (averages across epochs)\n",
    "    metrics_summary = final_results.groupby(\"Pathology\").mean()\n",
    "    print(\"\\nSummary of Metrics Across Epochs:\")\n",
    "    display(metrics_summary)\n",
    "\n",
    "    # Save the summary\n",
    "    metrics_summary.to_csv(os.path.join(checkpoint_dir, \"metrics_summary.csv\"), index=True)\n",
    "else:\n",
    "    print(\"No evaluation results to save.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_xrays",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
